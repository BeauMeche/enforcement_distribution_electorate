---
title: "Replication: 'The Distributive Politics of Enforcement'"
author: "Beau Meche"
date: "5/8/2020"
header-includes:
    - \usepackage{setspace}\doublespacing
output:
  bookdown::pdf_document2:
    extra_dependencies: ["rotating"]
bibliography: "bibliography1.bib"
#biblio-style: "chicagolike"
link_citations: true
toc: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = "!H",
                      out.height = "\\textheight",  
                      out.width = "\\textwidth")

# toolbox

# gt from github: 'remotes::install_github("rstudio/gt")'


library(tidyverse)
library(foreign)
library(tinytex)
library(gt)
library(bibtex)
library(robustHD)
library(rstanarm)
library(ggthemes)
library(stargazer)
library(huxtable)


```

# Abstract: 

"The Distributive Politics of Enforcement" by Alisha Holland (2014) analyzes electoral behavior's relationship with police action in opposition to low-income unlicensed street vendors in three cities in Latin America. I was successful in replicating the results, with minute variance due to apparent differences between regression output between R and Stata. In my extenstion I re-regressed the models from the original paper under Bayesian modeling methods in the interest of discovering any differences likely to arise. The regression outputs themselves were quite similar and model comparisons favored similar models to the author; however upon cross-validation model analysis I found that a majority of the models did contain 'problematic' values. This implies that the models showing statistically significant support for the author's claim do not effectively model a subset of specific cases should any one case be removed. Should this prove to be a problem, it is only a problem of outliers which are not surprising in a small dataset. Otherwise, I find that Bayesian analysis supports Holland's claims both in conclusion and in process. 



# Introduction and Overview: 

This paper[@Holland14] looks at police behavior in varying contexts around Latin America to explore what gets prosecuted and what is allowed to go unnoticed. The areas of interest are the relationships between constituency of the powers that be (at a given time) [fig. 1] and how much those individuals in the constituent groups care to have property rights enforced. Involving a great deal of original research, the author does an intriguing job of addressing this question in a way that I think could be complemented by additional Bayesian analysis, so I thought this would be a good project on a topic that I briefly studied previously and can now apply new skills to. 

The author argues that in many cases, not enforcing these laws is as intentional as enforcing them and makes an early assertion of one of her findings where she found up to a five times higher enforcement rate in places where constituents were primarily non-poor (a binary distinction make in the data analysis portion of the research) [fig.1]. Holland uses many standardized poisson regression models [fig. 2], which makes sense in the setting where response variables are the topic of interest. The catch in that model is the concern for invalidation by reverse causality. To counter this point, a supply vs. demand framework is raised as one might find in your favorite introductory economics course with the y-axis showing enforcement frequency and the x-axis showing the count of offenses. The key to this argument is in assuming that the intentional non-enforcement cases would translate to an outward shift in the ‘supply curve’ of enforcement. This framework gives a solid basis to analyze electoral and otherwise political behavior and regional differences. The results of the data and the proposed model seem to support this framework and initial intuition, even if it is specific to the case observed. Becuase of the specificity in political frameworks in question here, I would have concerns about the external validity of Holland’s models, but nonetheless I agree that the model results in support for the framework claiming that there is an inverse relationship between police enforcement of vending licensing and quantity of poor individuals voting in a district. 





```{r load_and_triage_data}

# this was the link code before I did the bibliography: 
# <a href="https://github.com/BeauMeche/enforcement_distribution_electorate">link</a>

# this project was done in STATA, so transcribing the code will likely be rough
# as my knowledge of STATA is negligible


# may as well take everything out of the zip file

unzip("data/Holland_dataverse_repl_01.zip")

enforcement <- read.dta("data/Enforcement.dta")

enforce1 <- read_tsv("data/Enforcement.tab",
                     col_types = cols(
  .default = col_double(),
  city = col_character(),
  district = col_character()))


```



# Literature Review:

The author of this paper has become relatively well known for her research and her findings in the realm of forbearance and electoral impacts relative to low income populations in Latin America. In the literature published after this particular paper in 2014, a majority of it has also been authored by Holland and so far as I can tell there are few contradictions to be seen. The setting of the cases observed in this research is one where vendors of various goods are 'guilty' of using public space to peddle their private enterprises. This gives the police forces the ability to, within the letter of the law, arrest or otherwise enforce the statute that they are unlicensed to the space in question and that they should vacate. The reality tangent to this is that these vendors are also voters, and they are unlikely to forget the political head of the forces inhibiting their livelihood. Holland cites Colombia's National Data Archive with respect to the aspect that these vendors often do tend to rely on thier 'unlicensed incomes' as they are usually quite low-ranking in the income distribution.

Without digressing into Holland's research or the other supporting research again, it seems intuitive that where there is a majority of voters comprised of these individuals in the lower income distribution range who may rely, or know someone who relies, on secondary income to sustain themselves you would find that the local government would be less harsh on these cases of unregistered free markets. In fact, since this paper was published other authors have found cases where the government actually incentivizes the collaboration or unionization of these unregistered individuals as a sort of subset of the economy [@Hummel17]. 

Overall this vein of inquiry has fascinating ramifications in electoral predictions and voter behavior, especially in cases where statistical analyses show decreased sensitivities to potentially exogenous enforcement factors like department budgets and other angles. Carefully constructed models slowly begin to point their fingers toward constituency as a large driver of enforcement behaviors; and while I would prefer to see a few more rigors in the process, I tend to be convinced by the findings available thus far.




# Extension: 

Fortunately, I believe that I was able to replicate the major findings from the original paper. In an effort to further explore^[[“All analysis and output code for this replication project as well as the original paper are available here.](https://github.com/BeauMeche/enforcement_distribution_electorate)] the findings from Holland’s work, the same poisson models that comprise ‘Figure 1’ were run through models built with intrinsic Bayesian methods. Because Bayesian inference and regression methods present different types of interpretation of uncertainty and offer a different analytical approach, these results have been compared to the original as a kind of sensitivity analysis with respect to statistical methodology. 

The first exploration was with respect to missing data. In short, there was no place for concern for missing data. Whether this is due to the pre-cleaned data being the only repository made available, the nature of the original research lending a low rate of missing values, or other factors is unknown. It was found to be the case that there were complete records funneled into all models constructed in the original study, and the only ‘missing’ values were variables completely present or missing at the city level. 

```{r extension_missing_data, message=F, warning=F}

## commented code here was used in feeling out the nature of the missing data
## Upon exploration, I realized that there is no real concern for said missing data in this case, only

library(mice)
library(naniar)

# enforce1 %>% 
#   filter(city == "santiago") %>% 
# miss_var_summary()

# selecting these because they are the only cases of NA values

# enforce1 %>% 
#   select(right, reports, arrests, salary, margin, employees) %>% 
#   gg_miss_var(show_pct = T) +
#   labs(title = "Variables with missing values:",
#        y = "Percent of values missing") +
#   scale_y_continuous(breaks = seq(0,70,5))

enforce1 %>% 
  select(right, reports, arrests, salary, 
         margin, employees, city) %>% 
  gg_miss_var(facet = city,
              show_pct = T) +
  labs(title = "Missing data by City: A Non-Issue",
       subtitle = "Percentage of data missing by variable. Any 'missing' cases
are 100% missing and comprise the list of varaibles excluded
from the model(s) for that city.",
       caption = "Original data collected by A. Holland",
y = "")

```

The second exploration was with regard to the Bayesian regression coefficients and whether they were notably different from the originals. On the whole, the coefficients seemed to be quite similar. The signs of the coefficients were certainly correct, and the actual estimates were either identical or close enough to attribute to rounding error or slight differences in statistical modeling across the transition from the original language ( Stata ) to the one used in this analysis ( R ). 

```{r, data_bogota_regression2, message=F, warning = F}

# table 2 partial replication w/ standardized coefficients
# mimicing the methods in the stata code file

# Bogota

tab2 <- enforce1 %>% 
  filter(city == "bogota") %>% 
  mutate(slower = standardize(x = lower, centerFun = mean, 
                              scaleFun = sd),
         svendors = standardize(x = vendors, centerFun = mean,
                                scaleFun = sd),
         sbudget = standardize(x = budget, 
                               centerFun = mean,
                               scaleFun = sd),
         spop = standardize(x = population,
                                   centerFun = mean,
                                   scaleFun = sd))


```

```{r data_lima_regressions, message=F, warning=F}

# tab2_lima

tab2l <- enforce1 %>% 
  filter(city == "lima") %>% 
  mutate(slower = standardize(x = lower, centerFun = mean, 
                            scaleFun = sd),
       svendors = standardize(x = vendors, centerFun = mean,
                              scaleFun = sd),
       sbudget = standardize(x = budget, 
                             centerFun = mean,
                             scaleFun = sd),
       spop = standardize(x = population,
                                 centerFun = mean,
                                 scaleFun = sd),
       smargin = standardize(x = margin, centerFun = mean,
                             scaleFun = sd),
       int_ML = lower * margin,
       s_int_ML = standardize(x = int_ML, centerFun = mean,
                              scaleFun = sd))


```

```{r data_santiago_regressions, warning=F, message=F, results='asis'}

# tab2_santiago

tab2s <- enforce1 %>% 
  filter(city == "santiago") %>% 
  mutate(slower = standardize(x = lower, centerFun = mean, 
                            scaleFun = sd),
       svendors = standardize(x = vendors, centerFun = mean,
                              scaleFun = sd),
       sbudget = standardize(x = budget, 
                             centerFun = mean,
                             scaleFun = sd),
       spop = standardize(x = population,
                                 centerFun = mean,
                                 scaleFun = sd),
       sreports = standardize(x = reports,
                                centerFun = mean,
                                scaleFun = sd),
       smargin = standardize(x = margin, centerFun = mean,
                             scaleFun = sd),
       int_ML = lower * margin,
       s_int_ML = standardize(x = int_ML, centerFun = mean,
                              scaleFun = sd),
       
       # the poisson regression requires integers as poisson models 
       # are based on counts
       
       arrest_T = as.integer(arrests * 1000))

```


```{r extension_Bayes_model_types}

# this will feed into regression tables like the previous one, only calculated
# with Bayesian methods through the rstanarm package and with predictions based 
# on the median values of the prediction distribution of the 1000 iterations
# implicit to Bayesian methods and to stan_glm specifically. 

# Bogota regression:

bogreg1 <- stan_glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2, family = poisson,refresh = 0)

# Lima regs:

limreg1 <- stan_glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2l, family = poisson, refresh = 0)

limreg2 <- stan_glm(operations ~ slower + svendors + sbudget + spop + smargin,
              data = tab2l, family = poisson, refresh = 0)

limreg3 <- stan_glm(operations ~ slower + svendors + sbudget + spop + smargin +
                     s_int_ML, 
                    data = tab2l, family = poisson, refresh = 0)

# Santiago Regs:

santreg1 <- stan_glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2s, family = poisson, refresh = 0)

santreg2 <- stan_glm(operations ~ slower + svendors + sbudget + spop + smargin,
              data = tab2s, family = poisson, refresh = 0)

santreg3 <- stan_glm(operations ~ slower + svendors + sbudget + spop + smargin +
                     s_int_ML,
              data = tab2s, family = poisson, refresh = 0)

santreg4 <- stan_glm(operations ~ slower + svendors + sbudget + spop + right,
              data = tab2s, family = poisson, refresh = 0)

# arrests were decimal values s.t. df$arrests[n] = n/1000, so I mutated
# class to integer and mulitplied to get the true counts for santiago without
# affecting the other graphics etc. that use that variable. 

santreg5 <- stan_glm(arrest_T ~ slower + sbudget + spop + right + sreports,
              data = tab2s, family = poisson, refresh = 0)


#           # ,
#           # limreg1, limreg2, limreg3,
#           # santreg1, santreg2, santreg3, santreg4, santreg5)
# 
# 
#           # ,
#           # title = "Bayesian Model Conversion: ",
#           # covariate.labels=c("Lower", "Vendors", "Budget", "Population", "Margin",
#           #                    "Margin*Lower", "Right", "Reports"),
#           # # float.env = "sidewaystable",
#           # no.space = TRUE,
#           # column.labels = c("Bogota", "Lima", "Santiago"),
#           # column.separate = c(1, 3, 5),
#           # type = "text")
# 
# 
# # summary(z, digits = 4)
# # summary(q, digits = 4)


```

\newpage

```{r bog_lim_table}

h_tab1 <- huxreg("(B 1)" = bogreg1, 
                 "(L 1)" = limreg1,
                 "(L 2)" = limreg2,
                 "(L 3)" = limreg3,
                 error_format = "({std.error})",
                 align = ".",
                 stars = NULL,
                 statistics = c('Num. Observations' = 'nobs'),
                 coefs = c("Intercept" = "(Intercept)",
                           "Lower" = "slower",
                           "Vendors" = "svendors",
                           "Budget" = "sbudget",
                           "Population" = "spop",
                           "Margin" = "smargin",
                           "Margin*Lower" = "s_int_ML")) %>% 
  set_caption("Bogota and Lima") %>% 
  add_footnote("B1, L1-3 indicate regression country / model number from the replicated regression model table. Numerically identified models contain mirrored variables, but are run through a Bayesian model.")

h_tab1

```

```{r santiago_h_table}

h_tab2 <- huxreg("(1)" = santreg1,
                 "(2)" = santreg2,
                 "(3)" = santreg3,
                 "(4)" = santreg4,
                 "(5)" = santreg5,
                 error_format = "({std.error})",
                 align = ".",
                 stars = NULL,
                 statistics = c('Num. Observations' = 'nobs'),
                 coefs = c("Intercept" = "(Intercept)",
                           "Lower" = "slower",
                           "Vendors" = "svendors",
                           "Budget" = "sbudget",
                           "Population" = "spop",
                           "Margin" = "smargin",
                           "Margin*Lower" = "s_int_ML")) %>% 
  set_caption("Santiago") %>% 
  add_footnote("(1)-(5) indicate model number from the replicated regression model table. Numerically identified models contain mirrored variables, but are run through a Bayesian model.")

h_tab2

```

The third inquiry was a test of the fit of the models. Using the leave-one-out cross validation method of analyzing the Bayesian regressions, there were between 3 and 7 observations per model in the regressions for Lima and Santiago that were deemed to be ‘problematic’ by this analysis. What this tells us is, when point $x$ is excluded from the data and the model re-fit, how well the model would predict said point. A probelmatic value then would imply cases where this extreme-case k-fold analysis indicates a poor prediction of the ommitted point by the model at hand. In small data sets with any level of variance, it seems reasonable at first to assume that there would be a few outlying values that would not be great fits to our model. With that said, too many problematic observations could become worrisome. See appendix for the endless iterations of model cross-checking. Taking the preferred model, by LOO compared standards, I also looked at the density overlay plots to observe similarity, or lack thereof, between the model and replicated data from the posterior distribution. 



## Comparing the Bayesian models to several replicated datasets as a visual test for model fit renders the following relationships: 

### Bogota: Regression 1

```{r pp_check_Bog, cache=F}

# Bogota

pp_check(bogreg1, plotfun = "dens_overlay")

```

### Lima: Regression 3

```{r pp_check_Lim, cache=F}

pp_check(limreg3, plotfun = "dens_overlay")

```

### Santiago: Regression 4

```{r pp_check_S4, cache=F}

pp_check(santreg4, plotfun = "dens_overlay")

```

### Santiago: Regression 5

* Regression 5 is a separate model construction and thus was not compared to models 1-4. 

```{r pp_check_S5, cache=F}

pp_check(santreg5, plotfun = "dens_overlay")

```



The final inquiry was with regard to ‘statistical significance’ and its equivalent in Bayesian inference through rstanarm [@rstanPKG] . This portion of the analysis was done by juxtaposing the frequestist confidence intervals with the Bayesian credible intervals for the model parameters. All confidence, credible intervals were evaluated at the $95\%$ significance level. As Bayesian analyses do not involve p-values and t-distributions, but rather posterior distributions of parameter estimates, I found it convenient to compare the intervals between the two model methodologies for a comparison of statistical significance. The results of this comparison were quire similar, with a great majority of values mirroring significance at the $95\%$ level, absolute values being relevantly similar, and sign matching. 

In conclusion, this study shows a substantial robustness to change in the model theory, as the statistical significance shows insubstantial variation. Replicating this study through Bayesian methods results in identical or quite similar regression coefficient values and error metrics where applicable. There may be room to explore the outlier cases, though more data would likely be difficult to obtain given the nature of the rigor of the original data acquisition. I have found no conclusions outside of or contrary to the study, and my own extension's findings stand in support of Holland's original work. 

\newpage

# Appendix:


```{r AP_figure3_TL, echo=FALSE}

top_left <- enforce1 %>% 
  mutate(poordistrict = as.integer(poordistrict)) %>% 
  filter(city == "bogota") %>% 
  select(poordistrict, operations, vendors)

fit_a <- stan_glm(data = top_left, operations ~ vendors, refresh = 0)

# this is the plot for Bogota

TL <- top_left %>% 
  ggplot(aes(y = operations, x = vendors)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 30), 
                     breaks = c(0, 5, 10, 15, 20, 25, 30)) +
  scale_x_continuous(limits = c(0, 12),
                     breaks = seq(0, 12, 2)) +
  geom_abline(slope = coef(fit_a)[2], 
              intercept = coef(fit_a)[1]) +
  labs(
    title = "",
    x = "Unlicensed Vendors (thousands), Bogota",
    y = "Police Operations",
    shape = "non-poor, poor")  + 
  theme(legend.position = "bottom")


# _______________________
# notes from meeting with Alice

#pch

#resize box / scale box

#piazza post about rescaling tables

```



```{r AP_fig3_TR, warning=F}

# missing value warning, unsure why, blocked for milestone 5

# this is the plot for Lima

top_right <- enforce1 %>% 
  mutate(poordistrict = as.integer(poordistrict)) %>% 
  filter(city == "lima") %>% 
  select(poordistrict, operations, vendors)

fit_b <- stan_glm(data = top_right, operations ~ vendors, refresh = 0)

TR <- top_right %>% 
  ggplot(aes(y = operations, x = vendors)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 80), 
                     breaks = seq(0,80, 20)) +
  scale_x_continuous(limits = c(0, 12),
                     breaks = seq(0, 12, 2)) +
  geom_abline(slope = coef(fit_b)[2], 
              intercept = coef(fit_b)[1]) +
  labs(
    title = "",
    x = "Unlicensed Vendors (thousands), Lima",
    y = "Police Operations",
    shape = "non-poor, poor")  + 
  theme(legend.position="bottom")

```

```{r AP_fig3_BL, echo=F}

# this is the plot for Santiago

bot_left <- enforce1 %>% 
  mutate(poordistrict = as.integer(poordistrict)) %>% 
  filter(city == "santiago") %>% 
  select(poordistrict, operations, vendors)

fit_c <- stan_glm(data = bot_left, operations ~ vendors, refresh = 0)

BL <- bot_left %>% 
  ggplot(aes(y = operations, x = vendors)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 20), 
                     breaks = seq(0,80, 20)) +
  scale_x_continuous(limits = c(0, 12),
                     breaks = seq(0, 12, 2)) +
  geom_abline(slope = coef(fit_c)[2], 
              intercept = coef(fit_c)[1]) +
  labs(
    title = "",
    x = "Unlicensed Vendors (thousands), Santiago",
    y = "Police Operations",
    shape = "non-poor, poor")  + 
  theme(legend.position="bottom")

```

```{r AP_fig3_BR, warning=F}

# this is the plot for Santiago (crime reports)

bot_right <- enforce1 %>% 
  mutate(poordistrict = as.integer(poordistrict)) %>% 
  filter(city == "santiago") %>% 
  select(poordistrict, arrests, reports)

fit_d <- stan_glm(data = bot_right, arrests ~ reports, refresh = 0)

BR <- bot_right %>% 
  ggplot(aes(y = arrests, x = reports)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 20), 
                     breaks = seq(0, 20, 5)) +
  scale_x_continuous(limits = c(0, 60),
                     breaks = seq(0, 60, 10)) +
  geom_abline(slope = coef(fit_d)[2], 
              intercept = coef(fit_d)[1]) +
  labs(
    title = "",
    x = "Crime Reports (thousands), Santiago",
    y = "Police Arrests (thousands)",
    shape = "non-poor, poor") + 
  theme(legend.position="bottom")

```

```{r beautiful_graph_redundancy, fig.align='left', eval=F, include=F}

# \newpage


#### "Beautiful" Graphic

b_graph <- top_left %>% 
  ggplot(aes(y = operations, x = vendors)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 30), 
                     breaks = c(0, 5, 10, 15, 20, 25, 30)) +
  scale_x_continuous(limits = c(0, 12),
                     breaks = seq(0, 12, 2)) +
  geom_abline(slope = coef(fit_a)[2], 
              intercept = coef(fit_a)[1],
              color = "blue") +
  labs(title = "Enforcing Vendors' Licenses: Bogotá, Columbia",
        subtitle = "Data showing the association between quantity of unlicensed street 
vendors and number of police operations in the area in question. A linear 
regression line is included to enphasize the slope of the reationship",
        caption = "Data from original research done by Alisha C. Holland. 
Find the original paper and reference to the associated 
dataverse at: https://doi.org/10.1111/ajps.12125",
        x = "Unlicensed Vendors (thousands), Bogotá",
        y = "Police Operations",
        shape = "Poor District Binary")  + 
  theme_hc() +
  theme(legend.position="bottom")

b_graph

```


```{r Begin_appendix_code, message=FALSE, warning=FALSE, include=F}

library(cowplot)

plot_grid(TL, TR); plot_grid(BL, BR)

```

```{r AP_figure_4_holland_rep}

# don't think I need the original for comparison any longer

# ![This is Holland's Figure 3 for reference: ]("holland_figure3_appendix_pic.png")

# this is all code in attempt to replicate the methods, which I understand, 
# used in the replication done in stata. 

# within figure 2, poisson regression predictions divided by the average predictions # is the y-axis value. I have managed to get some kind of prediction output, but 
# am currently unable to correlate that information back to the data in a usable/ 
# meaningful way. 

# this was a calculation for the mean operation count by city
# to check my hunch from the stata file... which was correct : )

op_mean <- enforce1 %>% 
  group_by(city) %>% 
  summarize(mean(operations)) %>% 
  arrange()
  

 # dummy code that was functional but irrelevant
# 
# mutate(exp_div_avg_ops = case_when(
#     city == "bogota" ~ operations/8.8947,
#     city == "lima" ~ operations/8.8947,
#     city == "santiago" ~ operations/8.8947)) %>%
#   ungroup() %>%
#   select(operations, exp_div_avg_ops, vendors, lower, city)

# created an average operations row

enforce1 <- enforce1 %>% 
  mutate(m.ops = mean(operations))

# mimicing the regression from the stata file (that is unlike the regression used
# in the regression table)

fit_f2 <- glm(operations ~ lower + vendors + budget + population,
              data = enforce1, family = poisson)

# the source code says to re-run the prediction code under various conditions, 
# this is my working attempt at this with "new" data as we have done in class
# and also how I understand predicted / expected value to work

new.lower1 <- data.frame(lower = seq(0,100,10),
                         vendors = mean(enforce1$vendors),
                         budget = mean(enforce1$budget),
                         population = mean(enforce1$population))

new.lower2 <- data.frame(lower = mean(enforce1$lower),
                         vendors = seq(0,15,1),
                         budget = mean(enforce1$budget),
                         population = mean(enforce1$population))

pred1 <- predict(fit_f2, newdata = new.lower1)

pred2 <- predict(fit_f2, newdata = new.lower2)

# q1 <- cbind(pred1, pred2)

# plot(q1)

    # error: number of rows of result is not a multiple of vector length (arg 1)

# output code that is ugly and thus kept out of the knit product

# plot(pred1)
# plot(pred2)
# print(fit_f2)

# plot(y = enforce1$operations, x = pred1)


```

```{r AP_freq_reg_Bogota, echo=F}

fit_t2 <- glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2, family = poisson)

```

```{r  AP_freq_regs_Lima, echo=F}

fit_t2_lim1 <- glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2l, family = poisson)

fit_t2_lim2 <- glm(operations ~ slower + svendors + sbudget + spop + smargin,
              data = tab2l, family = poisson)

fit_t2_lim3 <- glm(operations ~ slower + svendors + sbudget + spop + smargin +
                     s_int_ML,
              data = tab2l, family = poisson)

```

```{r AP_freq_regs_Santiago, echo=F}

fit_t2_s1 <- glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2s, family = poisson)

fit_t2_s2 <- glm(operations ~ slower + svendors + sbudget + spop + smargin,
              data = tab2s, family = poisson)

fit_t2_s3 <- glm(operations ~ slower + svendors + sbudget + spop + smargin +
                     s_int_ML,
              data = tab2s, family = poisson)

fit_t2_s4 <- glm(operations ~ slower + svendors + sbudget + spop + right,
              data = tab2s, family = poisson)

fit_t2_s5 <- glm(arrest_T ~ slower + sbudget + spop + right + sreports,
              data = tab2s, family = poisson)
```

```{r, holland_reg_replication, echo = F, warning=F, results='asis'}

stargazer(fit_t2,
          fit_t2_lim1, fit_t2_lim2, fit_t2_lim3,
          fit_t2_s1, fit_t2_s2, fit_t2_s3, fit_t2_s4, fit_t2_s5,
          title = "Replicated Models",
          covariate.labels=c("Lower", "Vendors", "Budget", "Population", "Margin",
                             "Margin*Lower", "Right", "Reports"),
          float.env = "sidewaystable", 
          no.space = TRUE,
          column.labels = c("Bogota", "Lima", "Santiago"),
          column.separate = c(1, 3, 5),
          header = F)

```

```{r loo_previous_models_lima, cache = F, message=F, fig.align='left', eval=F, echo=F}

# extension code here, the output was not fit for a table in the text, but 
# it functions here should anyong be curious


### Lima, Peru:

# this model had 3 problematic observations

looL1 <- loo(limreg1, k_threshold = 0.7)

# this model had 5 problematic observations

looL2 <- loo(limreg2, k_threshold = 0.7)

# this model had 7 problematic observations

looL3 <- loo(limreg3, k_threshold = 0.7)

# the third regression seems to be the best fit for Lima

lcL_1.2 <- loo_compare(looL1, looL2)

stargazer(lcL_1.2, type = "text")

stargazer(loo_compare(looL2, looL3), type = "text")

stargazer(loo_compare(looL1, looL3), type = "text")

# pp_check density overlay for model 3

```

```{r loo_prev_models_santiago, cache = F, message=F, eval=F, echo=F}

# extension code here, the output was not fit for a table in the text, but 
# it functions here should anyong be curious

# 5 problematic observations

looS1 <- loo(santreg1, k_threshold = 0.7)

# 6 problems...

looS2 <- loo(santreg2, k_threshold = 0.7)

# 7 problems...

looS3 <- loo(santreg3, k_threshold = 0.7)

# 7 problems...

looS4 <- loo(santreg4, k_threshold = 0.7)

# USE THIS LATER
# pp_check(santreg4, "dens_overlay")

looS5 <- loo(santreg5, k_threshold = 0.7)

```

```{r, echo=F, eval=F}

### Santiago, Chile:

stargazer(loo_compare(looS1, looS2), type = "text")


stargazer(loo_compare(looS1, looS3), type = "text")


stargazer(loo_compare(looS1, looS4), type = "text")


stargazer(loo_compare(looS2, looS3), type = "text")


stargazer(loo_compare(looS2, looS4), type = "text")


stargazer(loo_compare(looS3, looS4), type = "text")

# model 5 is a different regression w/ different dep. var

```

```{r fit_comparisons, message=F, echo = F, eval=F}

## Bayesian - Frequentist Fit Comparisons: 

# (c) In this section, the first stat line contains the model coefficients for the indicated model run with frequentist model construction and the subsequent stat lines show $95\%$ credible intervals for the Bayesian version of the same model. 

### Bogota Model #1



print(confint(fit_t2), digits = 3)

round(posterior_interval(bogreg1, prob = .95), 4)


### Lima Model #1

print(confint(fit_t2_lim1), digits = 3)

round(posterior_interval(limreg1, prob = .95), 4)


### Lima Model #2

print(confint(fit_t2_lim2), digits = 3)

round(posterior_interval(limreg2, prob = .95), 3)


### Lima Model #3

print(confint(fit_t2_lim3), digits = 3)

round(posterior_interval(limreg3, prob = .95), 3)

### Santiago Model #1

print(confint(fit_t2_s1), digits = 3)

round(posterior_interval(santreg1, prob = .95), 3)


### Santiago Model #2

print(confint(fit_t2_s2), digits = 3)

round(posterior_interval(santreg2, prob = .95), 3)

### Santiago Model #3

print(confint(fit_t2_s3), digits = 3)

round(posterior_interval(santreg3, prob = .95), 3)


### Santiago Model #4

print(confint(fit_t2_s4), digits = 3)

round(posterior_interval(santreg4, prob = .95), 3)


### Santiago Model #5
  
print(confint(fit_t2_s5), digits = 3)

round(posterior_interval(santreg5, prob = .95), 3)

```


