---
title: "Replication: 'The Distributive Politics of Enforcement'"
author: "Beau Meche"
date: "2/21/2020"
output:
  bookdown::pdf_document2:
    extra_dependencies: ["rotating"]
bibliography: "bibliography1.bib"
#biblio-style: "chicagolike"
link_citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = "!H",
                      out.height = "\\textheight",  
                      out.width = "\\textwidth")

# toolbox

# gt from github: 'remotes::install_github("rstudio/gt")'


library(tidyverse)
library(foreign)
library(tinytex)
library(gt)
library(bibtex)
library(rstanarm)
library(ggthemes)
library(stargazer)
library(huxtable)


```

#### Abstract: 

"The Distributive Politics of Enforcement" by Alisha Holland (2014) analyzes electoral behavior's relationship with police action in opposition to low-income unlicensed street vendors in three cities in Latin America. I was mostly successful in replicating the results, with minute variance due to apparent differences between regression output between R and Stata. In my extenstion I re-regressed the models from the original paper under Bayesian modeling methods in the interest of discovering any differences likely to arise. The regression outputs themselves were quite similar and model comparisons favored similar models to the author; however upon cross-validation model analysis I found that a majority of the models were laden with 'problematic' values. This implies that the models showing statistically significant support for the author's claim do not effectively model the original dataset should any one value be removed and further implies that caution should be taken with associated claims pending better modeling or more data.



#### Introduction and Overview: 

This paper[@Holland14] looks at police behavior in varying contexts around Latin America to explore what gets prosecuted and what is allowed to go unnoticed. The areas of interest are the relationships between constituency of the powers that be (at a given time) [fig. 1] and how much those individuals in the constituent groups care to have property rights enforced. Involving a great deal of original research, the author does an intriguing job of addressing this question in a way that I think could be complemented by additional Bayesian analysis, so I thought this would be a good project on a topic that I briefly studied previously and can now apply new skills to. 

The author argues that in many cases, not enforcing these laws is as intentional as enforcing them and makes an early assertion of one of her findings where she found up to a five times higher enforcement rate in places where constituents were primarily non-poor (a binary distinction make in the data analysis portion of the research) [fig.1]. Holland uses many standardized poisson regression models [fig. 2], which makes sense in the setting where response variables are the topic of interest. The catch in that model is the concern for invalidation by reverse causality. To counter this point, a supply vs. demand framework is raised as one might find in your favorite introductory economics course with the y-axis showing enforcement frequency and the x-axis showing the count of offenses. The key to this argument is in assuming that the intentional non-enforcement cases would translate to an outward shift in the ‘supply curve’ of enforcement. This framework gives a solid basis to analyze electoral and otherwise political behavior and regional differences. The results of the data and the proposed model seem to support this framework and initial intuition, even if it is specific to the case observed. Becuase of the specificity in political frameworks inquestion here, I would have concerns about the external validity of Holland’s models, but nonetheless I agree that the model results in support for the framework claiming that there is an inverse relationship between police enforcement of vending licensing and quantity of poor individuals voting in a district. 





```{r load_and_triage_data}

# this was the link code before I did the bibliography: 
# <a href="https://github.com/BeauMeche/enforcement_distribution_electorate">link</a>

# this project was done in STATA, so transcribing the code will likely be rough
# as my knowledge of STATA is negligible


# may as well take everything out of the zip file

unzip("Holland_dataverse_repl_01.zip")

enforcement <- read.dta("Enforcement.dta")

enforce1 <- read_tsv("Enforcement.tab",
                     col_types = cols(
  .default = col_double(),
  city = col_character(),
  district = col_character()))


```


```{r data_exploration, echo=FALSE}

# 
# \newpage
# #### Initial Data Exploration


# 
# # variable "lower" is the share of lower-class residents in a given district
# 
# glimpse(enforce1)
# 
# enforcement %>% 
#   filter(city == "lima") %>%
#   ggplot(aes(x = corruption, y = operations)) +
#   geom_point(aes(color = poor)) +
#   labs(
#     title = "Corruption Coefficients and Police Activity",
#     x = "Number of Operations",
#     y = "Corruption Scale",
#     color = "% Poor
# Population"
#   ) +
#   scale_x_continuous(breaks = (seq(0,20, by= 2)))

```





#### Literature Review

The author of this paper has become relatively well known for her research and her findings in the realm of forbearance and electoral impacts relative to low income populations in Latin America. In the literature published after this particular paper in 2014, a majority of it has also been authored by Holland and so far as I can tell there are few contradictions to be seen. The setting of the cases observed in this research is one where vendors of various goods are 'guilty' of using public space to peddle their private enterprises. This gives the police forces the ability to, within the letter of the law, arrest or otherwise enforce the statute that they are unlicensed to the space in question and that they should vacate. The reality tangent to this is that these vendors are also voters, and they are unlikely to forget the political head of the forces inhibiting their livelihood. Holland cites Colombia's National Data Archive with respect to the aspect that these vendors often do tend to rely on thier 'unlicensed incomes' as they are usually quite low-ranking in the income distribution.

Without digressing into Holland's research or the other supporting research again, it seems intuitive that where there is a majority of voters comprised of these individuals in the lower income distribution range who may rely, or know someone who relies, on secondary income to sustain themselves you would find that the local government would be less harsh on these cases of unregistered free markets. In fact, since this paper was published other authors have found cases where the government actually incentivizes the collaboration or unionization of these unregistered individuals as a sort of subset of the economy [@Hummel17]. 

Overall this vein of inquiry has fascinating ramifications in electoral predictions and voter behavior, especially in cases where statistical analyses show decreased sensitivities to potentially exogenous enforcement factors like department budgets and other angles. Carefully constructed models slowly begin to point their fingers toward constituency as a large driver of enforcement behaviors; and while I would prefer to see a few more rigors in the process, I tend to be convinced by the findings available thus far.




#### Extension: 

In effort to further explore^[[“All analysis and output code for this replication project as well as the original paper are available here.](https://github.com/BeauMeche/enforcement_distribution_electorate)] the findings from Holland’s work, the same poisson models that comprise ‘Figure 1’ were run through models built with intrinsic Bayesian methods. Because Bayesian inference and regression methods present different types of interpretation of uncertainty and offer a different analytical approach, these results have been compared to the original as a kind of sensitivity analysis with respect to statistical methodology. 

The first exploration was with respect to missing data. In short, there was no place for concern for missing data. Whether this is due to the pre-cleaned data being the only repository made available, the nature of the original research lending a low rate of missing values, or other factors is unknown. It was found to be the case that there were complete records funneled into all models constructed in the original study, and the only ‘missing’ values were variables completely present or missing at the city level. 

The second exploration was with regard to the Bayesian regression coefficients and whether they were notably different from the originals. On the whole, the coefficients seemed to be quite similar. The signs of the coefficients were certainly correct, and the actual estimates were either identical or close enough to attribute to rounding error or slight differences in statistical modeling across the transition from the original language ( Stata ) to the one used in this analysis ( R ). 

The third inquiry was a test of the fit of the models. Using the leave-one-out cross validation method of analyzing the Bayesian regressions, there were between 3 and 7 observations per model in the regressions for Lima and Santiago that were deemed to be ‘problematic’ by this analysis. In small data sets with any level of variance, it seems reasonable at first to assume that there would be a few outlying values that would not be great fits to the model at hand. However, in this case the gross number of observations available to the models are 34 or 36. This implies that if the model ignores or removes the extraneous observations (which is the case in the Bayesian methods employed [@rstanPKG] ) we range farther into territory in which the minimum value to employ the central limit theorem and various other underlying statistical assumptions. 

The final inquiry was with regard to ‘statistical significance’ and its equivalent in Bayesian inference through rstanarm [@rstanPKG] . This portion of the analysis is ongoing and the results are not yet rigorously completed enough to present for consumption. See final draft.

```{r mstone_4_gtTable, echo=FALSE}

# no longer need the gt table for this project
# this page will have the project extension proposal on it

# gt1 <- enforcement %>% 
#   filter(city == "santiago") %>% 
#   select(arrests, city, district, population) %>% 
#   # mutate(marrest = mean(arrests),
#   #        mcorrupt = mean(corruption),
#   #        mtax = mean(tax)) %>% 
#   mutate(arrpercap = round(arrests/population, 3)) %>% 
#   group_by(district) %>% 
#   drop_na() %>% 
#   summarise(avg_arrestPC = mean(arrpercap))
# 
# gt1 %>%
#   # mutate(avg_arrests = round(`mean(arrests)`, 2)) %>%
#   # select(!`mean(arrests)`) %>%
#   arrange(desc(avg_arrestPC)) %>%
#   gt() %>%
#   fmt_percent(columns = vars(avg_arrestPC)) %>%
#   cols_label(avg_arrestPC = "Arrest Rate",
#              district = "District") %>%
#   tab_header(
#     title = "Average Per Capita Arrest Rate per District",
#     subtitle = "Districts from the city of Santiago"
#     )


```



```{r regressions1, echo=FALSE}

# this text was the description of the latter contained code

# \newpage
# #### Regression Table Attempt
# Stata would be nicer most likely, but here goes...
# The data used can be found in my github repo [@BMeche_proj_repo].
# 
# All regressions are prediction models of the number of police operations, in this case all operations in lima. Particularly interesting is the regression (4), where the glm regression predicts a decrease of $\approx 28$ operations on average when the district is poor vs. when it is not... things seem to change when the core constituency changes. 

# attempting to use the stargazer package... we'll see how this goes
lima <- enforcement %>% 
  filter(city == "lima")


m1 <- lm(data = lima, operations ~ vendors)

m2 <- lm(data = lima, operations ~ vendors + lower)

m3 <- lm(data = lima, operations ~ vendors + lower + budget)

m4 <- glm(data = lima, operations ~ poordistrict)

# couldn't get the top lables to cooperate, but the meat of the exercise is here

# check out the binary regression (4)... poor constituents get less harassments... wild

```


```{r}

# stargazer(m1, m2, m3, m4, type = "text")

```


```{r figure3_TL, echo=FALSE}

top_left <- enforce1 %>% 
  mutate(poordistrict = as.integer(poordistrict)) %>% 
  filter(city == "bogota") %>% 
  select(poordistrict, operations, vendors)

fit_a <- stan_glm(data = top_left, operations ~ vendors, refresh = 0)

# this is the plot for Bogota

TL <- top_left %>% 
  ggplot(aes(y = operations, x = vendors)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 30), 
                     breaks = c(0, 5, 10, 15, 20, 25, 30)) +
  scale_x_continuous(limits = c(0, 12),
                     breaks = seq(0, 12, 2)) +
  geom_abline(slope = coef(fit_a)[2], 
              intercept = coef(fit_a)[1]) +
  labs(
    title = "",
    x = "Unlicensed Vendors (thousands), Bogota",
    y = "Police Operations",
    shape = "non-poor, poor")  + 
  theme(legend.position = "bottom")


# _______________________
# notes from meeting with Alice

#pch

#resize box / scale box

#piazza post about rescaling tables

```

```{r fake_data_reprex}

# fake data replication to resolve a ggplot query: ignore this code chunk


# library(tidyverse)
# 
# data(mtcars)
# 
# mtcars %>%  
#   ggplot(aes(y = cyl, x = hp)) + 
#   geom_point(aes(shape = factor(am))) + 
#   labs(x = "horse power rating",
#        y = "cylinders")
# 
# 
# 
#   
# library(tidyverse)
# 
# data(mtcars)
# 
# reprex(
# 
# mtcars %>%  
#   ggplot(aes(y = cyl, x = hp)) + 
#   geom_point(aes(shape = factor(am))) + 
#   labs(x = "horse power rating",
#        y = "cylinders")
# )


```



```{r fig3_TR, warning=F}

# missing value warning, unsure why, blocked for milestone 5

# this is the plot for Lima

top_right <- enforce1 %>% 
  mutate(poordistrict = as.integer(poordistrict)) %>% 
  filter(city == "lima") %>% 
  select(poordistrict, operations, vendors)

fit_b <- stan_glm(data = top_right, operations ~ vendors, refresh = 0)

TR <- top_right %>% 
  ggplot(aes(y = operations, x = vendors)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 80), 
                     breaks = seq(0,80, 20)) +
  scale_x_continuous(limits = c(0, 12),
                     breaks = seq(0, 12, 2)) +
  geom_abline(slope = coef(fit_b)[2], 
              intercept = coef(fit_b)[1]) +
  labs(
    title = "",
    x = "Unlicensed Vendors (thousands), Lima",
    y = "Police Operations",
    shape = "non-poor, poor")  + 
  theme(legend.position="bottom")

```

```{r fig3_BL, echo=F}

# this is the plot for Santiago

bot_left <- enforce1 %>% 
  mutate(poordistrict = as.integer(poordistrict)) %>% 
  filter(city == "santiago") %>% 
  select(poordistrict, operations, vendors)

fit_c <- stan_glm(data = bot_left, operations ~ vendors, refresh = 0)

BL <- bot_left %>% 
  ggplot(aes(y = operations, x = vendors)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 20), 
                     breaks = seq(0,80, 20)) +
  scale_x_continuous(limits = c(0, 12),
                     breaks = seq(0, 12, 2)) +
  geom_abline(slope = coef(fit_c)[2], 
              intercept = coef(fit_c)[1]) +
  labs(
    title = "",
    x = "Unlicensed Vendors (thousands), Santiago",
    y = "Police Operations",
    shape = "non-poor, poor")  + 
  theme(legend.position="bottom")

```

```{r fig3_BR, warning=F}

# this is the plot for Santiago (crime reports)

bot_right <- enforce1 %>% 
  mutate(poordistrict = as.integer(poordistrict)) %>% 
  filter(city == "santiago") %>% 
  select(poordistrict, arrests, reports)

fit_d <- stan_glm(data = bot_right, arrests ~ reports, refresh = 0)

BR <- bot_right %>% 
  ggplot(aes(y = arrests, x = reports)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 20), 
                     breaks = seq(0, 20, 5)) +
  scale_x_continuous(limits = c(0, 60),
                     breaks = seq(0, 60, 10)) +
  geom_abline(slope = coef(fit_d)[2], 
              intercept = coef(fit_d)[1]) +
  labs(
    title = "",
    x = "Crime Reports (thousands), Santiago",
    y = "Police Arrests (thousands)",
    shape = "non-poor, poor") + 
  theme(legend.position="bottom")

```

```{r, bogota_regression, message=F, warning = F}
# table 2 partial replication w/ standardized coefficients
# mimicing the methods in the stata code file

library(robustHD)

tab2 <- enforce1 %>% 
  filter(city == "bogota") %>% 
  mutate(slower = standardize(x = lower, centerFun = mean, 
                              scaleFun = sd),
         svendors = standardize(x = vendors, centerFun = mean,
                                scaleFun = sd),
         sbudget = standardize(x = budget, 
                               centerFun = mean,
                               scaleFun = sd),
         spop = standardize(x = population,
                                   centerFun = mean,
                                   scaleFun = sd))

fit_t2 <- glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2, family = poisson)

# stargazer(fit_t2, type = "text")

# poisson[] for reg model? 

# my.poisson.mod <- glm(fatalities ~ attack, family = "poisson", data = mydata)


```




```{r beautiful_graph_redundancy, fig.align='left', eval=F, echo=F}

# \newpage


#### "Beautiful" Graphic

b_graph <- top_left %>% 
  ggplot(aes(y = operations, x = vendors)) + 
  geom_point(aes(shape = factor(poordistrict))) + 
  scale_y_continuous(limits = c(0, 30), 
                     breaks = c(0, 5, 10, 15, 20, 25, 30)) +
  scale_x_continuous(limits = c(0, 12),
                     breaks = seq(0, 12, 2)) +
  geom_abline(slope = coef(fit_a)[2], 
              intercept = coef(fit_a)[1],
              color = "blue") +
  labs(title = "Enforcing Vendors' Licenses: Bogotá, Columbia",
        subtitle = "Data showing the association between quantity of unlicensed street 
vendors and number of police operations in the area in question. A linear 
regression line is included to enphasize the slope of the reationship",
        caption = "Data from original research done by Alisha C. Holland. 
Find the original paper and reference to the associated 
dataverse at: https://doi.org/10.1111/ajps.12125",
        x = "Unlicensed Vendors (thousands), Bogotá",
        y = "Police Operations",
        shape = "Poor District Binary")  + 
  theme_hc() +
  theme(legend.position="bottom")

b_graph

```


\newpage

#### Appendix: 



* Figure 1. 



```{r fig3_grid, message=F, warning=F}

library(cowplot)

plot_grid(TL, TR); plot_grid(BL, BR)

```


```{r figure_4_holland_rep}

# don't think I need the original for comparison any longer

# ![This is Holland's Figure 3 for reference: ]("holland_figure3_appendix_pic.png")

# this is all code in attempt to replicate the methods, which I understand, 
# used in the replication done in stata. 

# within figure 2, poisson regression predictions divided by the average predictions # is the y-axis value. I have managed to get some kind of prediction output, but 
# am currently unable to correlate that information back to the data in a usable/ 
# meaningful way. 

# this was a calculation for the mean operation count by city
# to check my hunch from the stata file... which was correct : )

op_mean <- enforce1 %>% 
  group_by(city) %>% 
  summarize(mean(operations)) %>% 
  arrange()
  

 # dummy code that was functional but irrelevant
# 
# mutate(exp_div_avg_ops = case_when(
#     city == "bogota" ~ operations/8.8947,
#     city == "lima" ~ operations/8.8947,
#     city == "santiago" ~ operations/8.8947)) %>%
#   ungroup() %>%
#   select(operations, exp_div_avg_ops, vendors, lower, city)

# created an average operations row

enforce1 <- enforce1 %>% 
  mutate(m.ops = mean(operations))

# mimicing the regression from the stata file (that is unlike the regression used
# in the regression table)

fit_f2 <- glm(operations ~ lower + vendors + budget + population,
              data = enforce1, family = poisson)

# the source code says to re-run the prediction code under various conditions, 
# this is my working attempt at this with "new" data as we have done in class
# and also how I understand predicted / expected value to work

new.lower1 <- data.frame(lower = seq(0,100,10),
                         vendors = mean(enforce1$vendors),
                         budget = mean(enforce1$budget),
                         population = mean(enforce1$population))

new.lower2 <- data.frame(lower = mean(enforce1$lower),
                         vendors = seq(0,15,1),
                         budget = mean(enforce1$budget),
                         population = mean(enforce1$population))

pred1 <- predict(fit_f2, newdata = new.lower1)

pred2 <- predict(fit_f2, newdata = new.lower2)

# q1 <- cbind(pred1, pred2)

# plot(q1)

    # error: number of rows of result is not a multiple of vector length (arg 1)

# output code that is ugly and thus kept out of the knit product

# plot(pred1)
# plot(pred2)
# print(fit_f2)

# plot(y = enforce1$operations, x = pred1)


```

```{r fig_2_placeholder, warning=F}

# removing message about removed rows

bog_mean <- pull(op_mean[1,2])

pt1 <- enforce1 %>%
  filter(lower >= 0 & lower <= 70) %>% 
  ggplot(aes(y = operations/bog_mean,
             x = lower)) +
  geom_smooth(aes(group = city,
                color = city),
                method = "glm",
                se = T) +
  scale_y_continuous(limits = c(0,5),
                     breaks = seq(0,5,1)) +
  scale_x_continuous(breaks = seq(0,70,20)) +
  labs(
    title = "The following graphs are my closest attempt at figure 2",
    y = "operations / average ops. in Bogota",
    x = "Percent lower class"
  )


pt2 <- enforce1 %>%
  filter(vendors >= 0 & vendors <= 12) %>% 
  ggplot(aes(y = operations/bog_mean,
             x = vendors)) +
  geom_smooth(aes(group = city,
                color = city),
                method = "glm",
                se = T) +
  scale_y_continuous(limits = c(0,5),
                     breaks = seq(0,5,1)) +
  scale_x_continuous(breaks = seq(0,12,3)) +
  labs(
    y = "operations / average ops. in Bogota",
    x = "Unlicensed Vendors (thousands)"
  )

# pt1; pt2


```

```{r, bogota_regression2, message=F, warning = F}
# table 2 partial replication w/ standardized coefficients
# mimicing the methods in the stata code file

library(robustHD)

tab2 <- enforce1 %>% 
  filter(city == "bogota") %>% 
  mutate(slower = standardize(x = lower, centerFun = mean, 
                              scaleFun = sd),
         svendors = standardize(x = vendors, centerFun = mean,
                                scaleFun = sd),
         sbudget = standardize(x = budget, 
                               centerFun = mean,
                               scaleFun = sd),
         spop = standardize(x = population,
                                   centerFun = mean,
                                   scaleFun = sd))

fit_t2 <- glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2, family = poisson)

# stargazer(fit_t2, type = "text")


```

```{r lima_regressions, message=F, warning=F}

# tab2_lima

tab2l <- enforce1 %>% 
  filter(city == "lima") %>% 
  mutate(slower = standardize(x = lower, centerFun = mean, 
                            scaleFun = sd),
       svendors = standardize(x = vendors, centerFun = mean,
                              scaleFun = sd),
       sbudget = standardize(x = budget, 
                             centerFun = mean,
                             scaleFun = sd),
       spop = standardize(x = population,
                                 centerFun = mean,
                                 scaleFun = sd),
       smargin = standardize(x = margin, centerFun = mean,
                             scaleFun = sd),
       int_ML = lower * margin,
       s.int_ML = standardize(x = int_ML, centerFun = mean,
                              scaleFun = sd))

fit_t2_lim1 <- glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2l, family = poisson)

fit_t2_lim2 <- glm(operations ~ slower + svendors + sbudget + spop + smargin,
              data = tab2l, family = poisson)

fit_t2_lim3 <- glm(operations ~ slower + svendors + sbudget + spop + smargin +
                     s.int_ML,
              data = tab2l, family = poisson)

# stargazer(fit_t2_lim1, fit_t2_lim2, fit_t2_lim3, type = "text")
  

```


* Figure 2


```{r santiago_regressions, warning=F, message=F, results='asis'}

# tab2_santiago

tab2s <- enforce1 %>% 
  filter(city == "santiago") %>% 
  mutate(slower = standardize(x = lower, centerFun = mean, 
                            scaleFun = sd),
       svendors = standardize(x = vendors, centerFun = mean,
                              scaleFun = sd),
       sbudget = standardize(x = budget, 
                             centerFun = mean,
                             scaleFun = sd),
       spop = standardize(x = population,
                                 centerFun = mean,
                                 scaleFun = sd),
       sreports = standardize(x = reports,
                                centerFun = mean,
                                scaleFun = sd),
       smargin = standardize(x = margin, centerFun = mean,
                             scaleFun = sd),
       int_ML = lower * margin,
       s.int_ML = standardize(x = int_ML, centerFun = mean,
                              scaleFun = sd),
       
       # the poisson regression requires integers as poisson models 
       # are based on counts
       
       arrest_T = as.integer(arrests * 1000))

fit_t2_s1 <- glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2s, family = poisson)

fit_t2_s2 <- glm(operations ~ slower + svendors + sbudget + spop + smargin,
              data = tab2s, family = poisson)

fit_t2_s3 <- glm(operations ~ slower + svendors + sbudget + spop + smargin +
                     s.int_ML,
              data = tab2s, family = poisson)

fit_t2_s4 <- glm(operations ~ slower + svendors + sbudget + spop + right,
              data = tab2s, family = poisson)

fit_t2_s5 <- glm(arrests ~ slower + sbudget + spop + right + sreports,
              data = tab2s, family = poisson)


stargazer(fit_t2,
          fit_t2_lim1, fit_t2_lim2, fit_t2_lim3,
          fit_t2_s1, fit_t2_s2, fit_t2_s3, fit_t2_s4, fit_t2_s5,
          title = "Replicated Models",
          covariate.labels=c("Lower", "Vendors", "Budget", "Population", "Margin",
                             "Margin*Lower", "Right", "Reports"),
          float.env = "sidewaystable", 
          no.space = TRUE,
          column.labels = c("Bogota", "Lima", "Santiago"),
          column.separate = c(1, 3, 5),
          header = F)

```

\newpage

* Figure 3 

With regard to missing data analysis, there were no points of concern as there were no regression analyses that incorporated missing data. below you can see the summary of data missing from the onset, but at the level of analyses, the individual cities, there were no calculations done on variables with incomplete sets of information. 

```{r extension_missing_data, message=F, warning=F}

## commented code here was used in feeling out the nature of the missing data
## Upon exploration, I realized that there is no real concern for said missing data in this case, only

library(mice)
library(naniar)

# enforce1 %>% 
#   filter(city == "santiago") %>% 
# miss_var_summary()

# selecting these because they are the only cases of NA values

# enforce1 %>% 
#   select(right, reports, arrests, salary, margin, employees) %>% 
#   gg_miss_var(show_pct = T) +
#   labs(title = "Variables with missing values:",
#        y = "Percent of values missing") +
#   scale_y_continuous(breaks = seq(0,70,5))

enforce1 %>% 
  select(right, reports, arrests, salary, 
         margin, employees, city) %>% 
  gg_miss_var(facet = city,
              show_pct = T) +
  labs(title = "Missing data by City",
       caption = "Original data collected by the author of the basis paper: A. Holland")

```

```{r extension_model_types}

# this will be a regression table like the previous one, only calculated with 
# Bayesian methods through the rstanarm package and with predictions based on 
# the median values of the prediction distribution of the 1000 iterations
# implicit to Bayesian methods and to stan_glm specifically. 

# Bogota regression: check out 'stan_glmer'
bogreg1 <- stan_glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2, family = poisson,refresh = 0)

limreg1 <- stan_glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2l, family = poisson, refresh = 0)

limreg2 <- stan_glm(operations ~ slower + svendors + sbudget + spop + smargin,
              data = tab2l, family = poisson, refresh = 0)

limreg3 <- stan_glm(operations ~ slower + svendors + sbudget + spop + smargin +
                     s.int_ML, 
                    data = tab2l, family = poisson, refresh = 0)

santreg1 <- stan_glm(operations ~ slower + svendors + sbudget + spop,
              data = tab2s, family = poisson, refresh = 0)

santreg2 <- stan_glm(operations ~ slower + svendors + sbudget + spop + smargin,
              data = tab2s, family = poisson, refresh = 0)

santreg3 <- stan_glm(operations ~ slower + svendors + sbudget + spop + smargin +
                     s.int_ML,
              data = tab2s, family = poisson, refresh = 0)

santreg4 <- stan_glm(operations ~ slower + svendors + sbudget + spop + right,
              data = tab2s, family = poisson, refresh = 0)

# arrests were decimal values s.t. df$arrests[n] = n/1000, so I mutated
# class to integer and mulitplied to get the true counts for santiago without
# affecting the other graphics etc. that use that variable. 

santreg5 <- stan_glm(arrest_T ~ slower + sbudget + spop + right + sreports,
              data = tab2s, family = poisson, refresh = 0)


# 
#           # ,
#           # limreg1, limreg2, limreg3,
#           # santreg1, santreg2, santreg3, santreg4, santreg5)
# 
# 
#           # ,
#           # title = "Bayesian Model Conversion: ",
#           # covariate.labels=c("Lower", "Vendors", "Budget", "Population", "Margin",
#           #                    "Margin*Lower", "Right", "Reports"),
#           # # float.env = "sidewaystable",
#           # no.space = TRUE,
#           # column.labels = c("Bogota", "Lima", "Santiago"),
#           # column.separate = c(1, 3, 5),
#           # type = "text")
# 
# 
# # summary(z, digits = 4)
# # summary(q, digits = 4)


```

\newpage
#### Extension: pt.2

(a) The object of this portion is to compare modeling methods between standard poisson regressions and those done in the Bayesian method. Errors, estimate measures, and fit metrics are slightly different, so we will see whether this methodological change brings up points of concern


(1) Bogota and Lima:

```{r bog_lim_table}

h_tab1 <- huxreg("(B 1)" = bogreg1, 
                 "(L 1)" = limreg1,
                 "(L 2)" = limreg2,
                 "(L 3)" = limreg3,
                 error_format = "({std.error})",
                 align = ".",
                 stars = NULL,
                 statistics = c('Num. Observations' = 'nobs'),
                 coefs = c("Intercept" = "(Intercept)",
                           "Lower" = "slower",
                           "Vendors" = "svendors",
                           "Budget" = "sbudget",
                           "Population" = "spop",
                           "Margin" = "smargin",
                           "Margin*Lower" = "s.int_ML")) %>% 
  set_caption("Bogota and Lima") %>% 
  add_footnote("B1, L1-3 indicate regression country / model number from the replicated regression model table")

h_tab1

```


(2) Santiago:

```{r santiago_h_table}

h_tab2 <- huxreg("(1)" = santreg1,
                 "(2)" = santreg2,
                 "(3)" = santreg3,
                 "(4)" = santreg4,
                 "(5)" = santreg5,
                 error_format = "({std.error})",
                 align = ".",
                 stars = NULL,
                 statistics = c('Num. Observations' = 'nobs'),
                 coefs = c("Intercept" = "(Intercept)",
                           "Lower" = "slower",
                           "Vendors" = "svendors",
                           "Budget" = "sbudget",
                           "Population" = "spop",
                           "Margin" = "smargin",
                           "Margin*Lower" = "s.int_ML")) %>% 
  set_caption("Santiago")

h_tab2

```





\newpage

(b) This section contains leave-one-out cross-validation model comparisons between the Bayesian models from part 'a' for cities with multiple regressions in the original analysis. 

 
```{r loo_previous_models_lima, cache = T, message=F, fig.align='left'}

# this model had 3 problematic observations

looL1 <- loo(limreg1, k_threshold = 0.7)

# this model had 5 problematic observations

looL2 <- loo(limreg2, k_threshold = 0.7)

# this model had 7 problematic observations

looL3 <- loo(limreg3, k_threshold = 0.7)

# the third regression seems to be the best fit for Lima

zzz <- loo_compare(looL1, looL2)

stargazer(zzz, type = "text")

```


```{r, fig.align='left'}

hux(zzz, add_colnames = T)

```

```{r}

loo_compare(looL2, looL3)

loo_compare(looL1, looL3)

# check density overlay for model 3

```

```{r}

stargazer(zzz, type = "text")

```




```{r loo_prev_models_santiago, cache = T, message=F}

# 5 problematic observations

looS1 <- loo(santreg1, k_threshold = 0.7)

# 6 problems...

looS2 <- loo(santreg2, k_threshold = 0.7)

# 7 problems

looS3 <- loo(santreg3, k_threshold = 0.7)

# 7 problems

looS4 <- loo(santreg4, k_threshold = 0.7)

# USE THIS VVV
# pp_check(santreg4, "dens_overlay")

looS5 <- loo(santreg5, k_threshold = 0.7)

loo_compare(looS1, looS2)

loo_compare(looS1, looS3)

loo_compare(looS1, looS4)

loo_compare(looS2, looS3)

loo_compare(looS2, looS4)

loo_compare(looS3, looS4)

    # model 5 is a different regression w/ different dep. var
    # loo_compare(looS1, looS5)
    
    # loo_compare(looS3, looS5)
    # 
    # loo_compare(looS4, looS5)
    # 
    # loo_compare(looS2, looS5)

```

```{r model_fit_comparisons, echo = T}

# time allowing: make a loop to average the median values for all observations

# Bogota: 

  print(coef(fit_t2), digits = 3)
  
  pred_b1 <- posterior_linpred(bogreg1)
  summary(pred_b1[1,])


# Lima 1

  print(coef(fit_t2_lim1), digits = 3)
  
  pred_l1 <- posterior_linpred(limreg1)
  summary(pred_l1[1,])
  
# Lima 2

  print(coef(fit_t2_lim2), digits = 3)
  
  pred_l2 <- posterior_linpred(limreg2)
  summary(pred_l2[1,])
  
# Lima 3
  
  print(coef(fit_t2_lim3), digits = 3)
  
  pred_l3 <- posterior_linpred(limreg3)
  summary(pred_l3[1,])
  
  
# Santiago 1
  
  print(coef(fit_t2_s1), digits = 3)
  
  pred_s1 <- posterior_linpred(santreg1)
  summary(pred_s1[1,])
  
# Santiago 2
  
  print(coef(fit_t2_s2), digits = 3)
  
  pred_s2 <- posterior_linpred(santreg1)
  summary(pred_s2[1,])
  
# Santiago 3
  
  print(coef(fit_t2_s3), digits = 3)
  
  pred_s3 <- posterior_linpred(santreg1)
  summary(pred_s3[1,])
  
# Santiago 4
  
  print(coef(fit_t2_s4), digits = 3)
  
  pred_s4 <- posterior_linpred(santreg1)
  summary(pred_s4[1,])
  
# Santiago 5
  
  print(coef(fit_t2_s5), digits = 3)
  
  pred_s5 <- posterior_linpred(santreg1)
  summary(pred_s5[1,])

```
\newpage
#### Model checks done on preferred competing models. <br\>



###### Bogota: Regression 1
```{r pp_check_Bog, cache=T}

# Bogota

pp_check(bogreg1, plotfun = "dens_overlay")

```
\newpage
###### Lima: Regression 3

```{r pp_check_Lim, cache=T}

pp_check(limreg3, plotfun = "dens_overlay")

```


\newpage
###### Santiago: Regression 4

```{r pp_check_S4, cache=T}

pp_check(santreg4, plotfun = "dens_overlay")

```

\newpage
###### Santiago: Regression 5
######## Regression 5 is a separate model construction and thus was not compared to models 1-4. 

```{r pp_check_S5, cache=T}

pp_check(santreg5, plotfun = "dens_overlay")

```










